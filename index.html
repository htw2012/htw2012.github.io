<!doctype html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, next" />








  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="天下之至柔，驰骋天下之至坚">
<meta property="og:type" content="website">
<meta property="og:title" content="数据娃嚼AI">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="数据娃嚼AI">
<meta property="og:description" content="天下之至柔，驰骋天下之至坚">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="数据娃嚼AI">
<meta name="twitter:description" content="天下之至柔，驰骋天下之至坚">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '6396577170768004000',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title> 数据娃嚼AI </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?c1f23d28bc4feb2545f94edbe083f29c";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>










  
  
    
  

  <div class="container one-collumn sidebar-position-left 
   page-home 
 ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">数据娃嚼AI</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/12/tool-tdm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/08/12/tool-tdm/" itemprop="url">
                  TDM-深度树匹配
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-12T10:50:37+08:00">
                2019-08-12
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tool/" itemprop="url" rel="index">
                    <span itemprop="name">Tool</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tool/match/" itemprop="url" rel="index">
                    <span itemprop="name">match</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/08/12/tool-tdm/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/08/12/tool-tdm/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>之前介绍过基于乘积量化方式PQ构建分库索引的<a href="https://htw2012.github.io/2019/07/30/tool-faiss/" target="_blank" rel="external">fasis工具</a>解决召回效率低的问题，本文介绍一种基于树的高效匹配算法。我们在数据结构上知道搜索二叉树BST等系列的树查找时间复杂度是对数级别，knn based的一些检索结构KD树等都是检索比较高效的数据结构，通过分而治之的方式进行不断查找。基于树这样的天生的优良特性，阿里妈妈的作者们在推荐领域下，提出TDM算法解决全库检索效率低和推荐系统两阶段分割的问题。针对召回和排序两阶段联合为一个阶段是目前的一个大趋势，而本文重点关注的是如何高效地进行全库检索。下文为个人解读的主要梳理部分</p>
<p>问题背景：某某场景下，加速全库匹配过程</p>
<p>文章：</p>
<p>TDM一期：Learning Tree-based Deep Model for Recommender Systems</p>
<p>TDM二期：Joint Optimization of Tree-based Index and Deep Model for Recommender Systems</p>
<p>代码：<a href="https://github.com/alibaba/x-deeplearning/tree/master/xdl-algorithm-solution/TDM" target="_blank" rel="external">https://github.com/alibaba/x-deeplearning/tree/master/xdl-algorithm-solution/TDM</a></p>
<ul>
<li><p>离线训练： <a href="https://github.com/alibaba/x-deeplearning/wiki/深度树匹配模型(TDM" target="_blank" rel="external">https://github.com/alibaba/x-deeplearning/wiki/深度树匹配模型(TDM</a>) </p>
</li>
<li><p>在线Serving：<a href="https://github.com/alibaba/x-deeplearning/wiki/TDMServing" target="_blank" rel="external">https://github.com/alibaba/x-deeplearning/wiki/TDMServing</a> </p>
</li>
</ul>
<p>来源：参考人脑，兴趣的建立由粗到细的组织方式和检索方法，比如10亿的商品列表，只需要30次的查找  </p>
<p>How: 为什么检索出来的top-k，就是用户感兴趣的 Topk?有效性如何去保证？有效性检索的建模背后隐藏着对用户兴趣的建模。</p>
<p>基础结构：用户兴趣的最大堆树，首先是定义第j层用户对节点n的兴趣为用户对对节点n的子节点层下j+1的兴趣最大值。</p>
<p><img src="images/image-20190823113809654.png" alt="image-20190823113809654"></p>
<p>由于是递归定义，具有性质：最大堆树下，当前层最优 TopK 节点的父亲，一定属于上一层的最优 TopK。</p>
<p>扩展点：这里的max操作可以如何去替换？min, all??</p>
<p>举例如下：</p>
<p><img src="images/image-20190823114344868.png" alt="image-20190823114344868"></p>
<p>如果item6和item8是全局的最优top2节点，那么SN层中SN3和SN4是最优的top2.</p>
<p>由此可见，用户兴趣的最大堆树的定义是保证这种检索(beam seach)有效的充分条件，所以实际做的过程可以从根节点出发，逐层选择top-k,一直到叶子节点。</p>
<p>既然最大堆树的定义保证这种检索的有效性，那么这棵树应该如何去学习？</p>
<p>从检索本质看，针对具体的某一层，beam search检索过程需要保证当前层检索层具有top k排序的能力。</p>
<p>整体的思路：构建符合这样性质的样本，让<strong>样本牵引模型学习</strong>，去逼近最大堆。</p>
<p>具体的做法：主要分叶子层的节点兴趣和中间层的节点兴趣两部分进行构建。叶子层的节点兴趣，从用户的直接行为产生，对应着感兴趣和不感兴趣。中间层的兴趣节点，<strong>用最大堆递归上述的方式去推导每一层的序标签</strong>，当我们有了每一层的序标签，就可以用深度学习去拟合序标签的样本。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/08/02/tf-crf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/08/02/tf-crf/" itemprop="url">
                  TensorFlow中使用CRF
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-08-02T16:08:36+08:00">
                2019-08-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tool/" itemprop="url" rel="index">
                    <span itemprop="name">Tool</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tool/CRF/" itemprop="url" rel="index">
                    <span itemprop="name">CRF</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/08/02/tf-crf/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/08/02/tf-crf/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>背景：关于CRF的应用，尤其是在深度学习之前它是一个nlp序列建模的比较主流方法，即使是深度学习大行其道，也会出现crf的影子，比如bilstm+crf, bert+crf.  关于crf的原理可参考众多的资料，本文提供一个在tensorflow中使用crf的一个简要概述。</p>
<h3 id="CRF使用的主要API"><a href="#CRF使用的主要API" class="headerlink" title="CRF使用的主要API"></a>CRF使用的主要API</h3><ul>
<li><p>crf_log_likelihood</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">def crf_log_likelihood(inputs,</div><div class="line">                       tag_indices,</div><div class="line">                       sequence_lengths,</div><div class="line">                       transition_params=None):</div><div class="line">  &quot;&quot;&quot;Computes the log-likelihood of tag sequences in a CRF.</div><div class="line"></div><div class="line">  Args:</div><div class="line">    inputs: A [batch_size, max_seq_len, num_tags] tensor of unary potentials</div><div class="line">        to use as input to the CRF layer.</div><div class="line">    tag_indices: A [batch_size, max_seq_len] matrix of tag indices for which we</div><div class="line">        compute the log-likelihood.</div><div class="line">    sequence_lengths: A [batch_size] vector of true sequence lengths.</div><div class="line">    transition_params: A [num_tags, num_tags] transition matrix, if available.</div><div class="line">  Returns:</div><div class="line">    log_likelihood: A [batch_size] `Tensor` containing the log-likelihood of</div><div class="line">      each example, given the sequence of tag indices.</div><div class="line">    transition_params: A [num_tags, num_tags] transition matrix. This is either</div><div class="line">        provided by the caller or created in this function.</div><div class="line">  &quot;&quot;&quot;</div></pre></td></tr></table></figure>
<p>输入：</p>
<ul>
<li>inputs，一元势能得分，针对每个word级别，每个标签的预测概率值，&lt;句子长度,标签大小&gt;的tensor</li>
<li>tag_indices：真实标签的序列，&lt;句子长度,标签大小&gt;的tensor</li>
<li>sequence_lengths：实际标签序列的长度，为一个值</li>
<li>transition_params：标签状态转移矩阵，学习的参数矩阵，可以预先给定</li>
</ul>
<p>输出：</p>
<ul>
<li>log_likelihood，word级别的对数似然概率</li>
<li>transition_params：学习后的状态转移矩阵</li>
</ul>
</li>
<li><p>解码过程的两个可用的API</p>
<ul>
<li>tf.contrib.crf.viterbi_decode(tf_unary<em>scores</em>, tf_transition_params) </li>
<li>tf.contrib.crf.crf_decode(unary_scores, transition_params, sequence_lengths) </li>
</ul>
</li>
<li><p>一个具体的例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># Data settings.</span></div><div class="line">num_examples = <span class="number">10</span></div><div class="line">num_words = <span class="number">20</span></div><div class="line">num_features = <span class="number">100</span></div><div class="line">num_tags = <span class="number">5</span></div><div class="line"></div><div class="line"><span class="comment"># Random features.</span></div><div class="line">x = np.random.rand(num_examples, num_words, num_features).astype(np.float32)</div><div class="line"></div><div class="line"><span class="comment"># Random tag indices representing the gold sequence.</span></div><div class="line">y = np.random.randint(num_tags, size=[num_examples, num_words]).astype(np.int32)</div><div class="line"></div><div class="line"><span class="comment"># All sequences in this example have the same length, but they can be variable in a real model.</span></div><div class="line">sequence_lengths = np.full(num_examples, num_words - <span class="number">1</span>, dtype=np.int32)</div><div class="line"></div><div class="line"><span class="comment"># Train and evaluate the model.</span></div><div class="line"><span class="keyword">with</span> tf.Graph().as_default():</div><div class="line">  <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</div><div class="line">    <span class="comment"># Add the data to the TensorFlow graph.</span></div><div class="line">    x_t = tf.constant(x)</div><div class="line">    y_t = tf.constant(y)</div><div class="line">    sequence_lengths_t = tf.constant(sequence_lengths)</div><div class="line"></div><div class="line">    <span class="comment"># Compute unary scores from a linear layer.</span></div><div class="line">    weights = tf.get_variable(<span class="string">"weights"</span>, [num_features, num_tags])</div><div class="line">    matricized_x_t = tf.reshape(x_t, [<span class="number">-1</span>, num_features])</div><div class="line">    matricized_unary_scores = tf.matmul(matricized_x_t, weights)</div><div class="line">    unary_scores = tf.reshape(matricized_unary_scores,</div><div class="line">                              [num_examples, num_words, num_tags])</div><div class="line"></div><div class="line">    <span class="comment"># Compute the log-likelihood of the gold sequences and keep the transition</span></div><div class="line">    <span class="comment"># params for inference at test time.</span></div><div class="line">    log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(</div><div class="line">        unary_scores, y_t, sequence_lengths_t)</div><div class="line"></div><div class="line">    <span class="comment"># Compute the viterbi sequence and score.</span></div><div class="line">    viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(</div><div class="line">        unary_scores, transition_params, sequence_lengths_t)</div><div class="line"></div><div class="line">    <span class="comment"># Add a training op to tune the parameters.</span></div><div class="line">    loss = tf.reduce_mean(-log_likelihood)</div><div class="line">    train_op = tf.train.GradientDescentOptimizer(<span class="number">0.01</span>).minimize(loss)</div><div class="line"></div><div class="line">    session.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">    mask = (np.expand_dims(np.arange(num_words), axis=<span class="number">0</span>) &lt;</div><div class="line">            np.expand_dims(sequence_lengths, axis=<span class="number">1</span>))</div><div class="line">    total_labels = np.sum(sequence_lengths)</div><div class="line"></div><div class="line">    <span class="comment"># Train for a fixed number of iterations.</span></div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1000</span>):</div><div class="line">      tf_viterbi_sequence, _ = session.run([viterbi_sequence, train_op])</div><div class="line">      <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</div><div class="line">        correct_labels = np.sum((y == tf_viterbi_sequence) * mask)</div><div class="line">        accuracy = <span class="number">100.0</span> * correct_labels / float(total_labels)</div><div class="line">        print(<span class="string">"Accuracy: %.2f%%"</span> % accuracy)</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="CRF-编码-训练逻辑"><a href="#CRF-编码-训练逻辑" class="headerlink" title="CRF 编码/训练逻辑"></a>CRF 编码/训练逻辑</h3><ul>
<li><p>获得表示：获得原始的model产出，比如lstm或者bert的表示，&lt;句子长度,隐藏大小&gt;的tensor.</p>
</li>
<li><p>计算每个word得分/全局的一元势函数(global <strong>unary potential</strong>)：加入Project映射层计算每个word的得分，输出的大小为&lt;句子长度,标签大小&gt;</p>
</li>
<li><p>Train: 使用crf_log_likelihood进行极大似然估计的train,这里可以标签的转移矩阵需要学习，是一个参数矩阵，得到一个log_likelihood和标签状态转移表示(decode用)</p>
<p>log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(<br>unary_scores, gold_tags, sequence_lengths, trans)</p>
</li>
<li><p>在word级别上累加计算loss</p>
<p>loss = tf.reduce_mean(-log_likelihood)</p>
</li>
<li><p>选择具体的优化算法进行学习</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">train_op = tf.train.GradientDescentOptimizer(0.01).minimize(loss)</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="CRF-解码-测试逻辑"><a href="#CRF-解码-测试逻辑" class="headerlink" title="CRF 解码/测试逻辑"></a>CRF 解码/测试逻辑</h3><p>一种可以使用crf_decode的tf代码进行解码，另外一种使用Numpy代码的viterbi_decode解码方式</p>
<p>numpy的解码风格:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">tf_unary_scores, tf_sequence_lengths, tf_transition_params, _ = session.run([unary_scores, sequence_lengths, transition_params, train_op])</div><div class="line"><span class="keyword">for</span> tf_unary_scores_, tf_sequence_length_ <span class="keyword">in</span> zip(tf_unary_scores, tf_sequence_lengths):</div><div class="line">    <span class="comment"># Remove padding.</span></div><div class="line">    tf_unary_scores_ = tf_unary_scores_[:tf_sequence_length_]</div><div class="line">    <span class="comment"># Compute the highest score and its tag sequence.</span></div><div class="line">    tf_viterbi_sequence, tf_viterbi_score = tf.contrib.crf.viterbi_decode(</div><div class="line">        tf_unary_scores_, tf_transition_params)</div></pre></td></tr></table></figure>
<p>TF的风格</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">viterbi_sequence, viterbi_score = tf.contrib.crf.crf_decode(unary_scores, transition_params, sequence_lengths)</div><div class="line"></div><div class="line">tf_viterbi_sequence, tf_viterbi_score, _ = session.run([viterbi_sequence, viterbi_score, train_op])</div></pre></td></tr></table></figure>
<p>主要逻辑：</p>
<ul>
<li><p>通过crf正向过程获得标签状态转移的参数transition_params。</p>
<p>log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(<br>unary_scores, gold_tags, sequence_lengths)</p>
</li>
<li><p>尝试上面介绍的任意一种解码的风格，输入参数都是transition_params和unary_scores一元势能函数，得到具体的解码序列</p>
</li>
</ul>
<p>要点：</p>
<p>1.transition_params标都是来自于crf_log_likelihood的第二个输出值</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">log_likelihood, transition_params = tf.contrib.crf.crf_log_likelihood(</div><div class="line">    unary_scores, gold_tags, sequence_lengths)</div></pre></td></tr></table></figure>
<p>2.unary_scores，是针对每个词的输入的得分(一元势能函数得分)，维度大小是&lt;句子长度, 标签大小&gt;，一般来源都是经过了一个project层/全联接层，比如</p>
<ul>
<li>bi-lstm, 输出为 &lt;句子长度,2倍隐藏大小&gt;, 这时需要一个映射层<2倍隐藏大小,标签大小>，转化为针对每个word的&lt;句子长度, 标签大小&gt;.</2倍隐藏大小,标签大小></li>
<li>Bert,输出为&lt;句子长度，隐层大小768&gt;, 这时需要一个映射层&lt;隐层大小,标签大小&gt;，转化为针对每个word的&lt;句子长度, 标签大小&gt;.</li>
</ul>
<p>3.训练过程和测试/解码过程都需要传入句子的实际长度的参数，主要是针对输入的长度进行了padding为最长，测试统计的时候要去掉这个部分。</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/30/tool-faiss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/07/30/tool-faiss/" itemprop="url">
                  相似性搜索工具-Faiss
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-30T15:32:52+08:00">
                2019-07-30
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tool/" itemprop="url" rel="index">
                    <span itemprop="name">Tool</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tool/match/" itemprop="url" rel="index">
                    <span itemprop="name">match</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/30/tool-faiss/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/07/30/tool-faiss/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="Faiss"><a href="#Faiss" class="headerlink" title="Faiss"></a>Faiss</h3><h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>目标：在千万规模的数据上，高效计算内积/相似性，返回top-K个结果</p>
<p>文章：Billion-scale similarity search with GPUs</p>
<p>代码：<a href="https://github.com/facebookresearch/faiss" target="_blank" rel="external">https://github.com/facebookresearch/faiss</a></p>
<p>简介：</p>
<ul>
<li>时间、质量和训练速度的权衡。</li>
<li>Faiss 是一个用于有效的相似性搜索和密集向量聚类的库。其包含了在任何大小（甚至可以大到装不进 RAM）的向量集中进行搜索的算法。其也包含用于评估和参数调整的支持性代码。</li>
<li>Faiss 是围绕一种存储了一个向量集的索引类型（index type）而构建的，并且提供了一个使用 L2 或点积向量比较在其中进行搜索的函数。</li>
</ul>
<p>方法：提出了一种用于 k-selection 的设计，其可以以高达理论峰值性能 55% 的速度进行运算，从而实现了比之前最佳的 GPU 方法快 8.5 倍的最近邻KNN搜索。另外，基于积量化（product quantization）的暴力计算、近似和压缩域搜索（compressed-domain search）提出优化过的设计，从而将其应用到了不同的相似性搜索场景中。</p>
<p>效果： 35 分钟内从 Yfcc100M 数据集的 9500 万张图像上构建一个高准确度的 k-NN 图（graph），也可以在 12 个小时内在 4 个 Maxwell Titan X GPU 上构建一个连接了 10 亿个向量的图。</p>
<p>实现细节：</p>
<ul>
<li><p>WarpSelect：文中提出的 k-selection 的设计，完全在寄存器（register）中维持状态，且仅需要在数据上进行单次通过，从而避免了 cross-warp synchronization，使用merge-odd 和 sort-odd 作为原语。</p>
<p>WarpSelect的整体流程如下：</p>
<p><img src="images/image-20190730073228959.png" alt="image-20190730073228959"></p>
</li>
<li><p>针对特定lane j的流程如下：       <img src="images/image-20190730073429119.png" alt="image-20190730073429119"></p>
</li>
</ul>
<h4 id="使用方法"><a href="#使用方法" class="headerlink" title="使用方法"></a>使用方法</h4><ul>
<li>安装：pip install faiss-cpu</li>
<li><p>使用示例：<a href="https://github.com/facebookresearch/faiss/wiki/Getting-started" target="_blank" rel="external">https://github.com/facebookresearch/faiss/wiki/Getting-started</a></p>
<ul>
<li>准备数据</li>
<li>构建索引：可以选择的索引格式为<a href="https://github.com/facebookresearch/faiss/wiki/Faiss-indexes" target="_blank" rel="external">https://github.com/facebookresearch/faiss/wiki/Faiss-indexes</a></li>
<li>搜索查询：search(query, top-k)</li>
</ul>
</li>
<li><p>加速搜索的一些技巧：<a href="https://github.com/facebookresearch/faiss/wiki/Faster-search" target="_blank" rel="external">https://github.com/facebookresearch/faiss/wiki/Faster-search</a></p>
<ul>
<li>使用复合的索引：<a href="https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)" target="_blank" rel="external">https://github.com/facebookresearch/faiss/wiki/Faiss-indexes-(composite)</a></li>
<li>IndexFlatL2 和 IndexIVFFlat都要存储所有的向量数据，对于大型数据集是不现实的, Faiss基于PQ提供了变体<strong>IndexIVFPQ</strong>来压缩数据向量（一定的精度损耗）</li>
</ul>
</li>
</ul>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><ul>
<li><a href="https://www.cnblogs.com/houkai/p/9316129.html" target="_blank" rel="external">Faiss教程：入门</a></li>
<li><p>Faiss的基础使用：<a href="https://waltyou.github.io/Faiss-Introduce/" target="_blank" rel="external">https://waltyou.github.io/Faiss-Introduce/</a></p>
</li>
<li><p>Faiss Indexs 的进一步了解：<a href="https://waltyou.github.io/Faiss-Indexs/" target="_blank" rel="external">https://waltyou.github.io/Faiss-Indexs/</a></p>
</li>
<li><p>向量检索在闲鱼视频去重的实践：<a href="https://zhuanlan.zhihu.com/p/43972326" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/43972326</a></p>
</li>
<li><p>阿里BE引擎深度集成开源的KNN库–FAISS<br>改造定制使其支持向量索引的分布式构建和查询，实现多种基于量化的方法如粗量化、积量化以及粗量化 + 积量化的组合等方法，并且在线查询的延时、索引构建的性能都很优秀。</p>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/21/ml-bo/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/07/21/ml-bo/" itemprop="url">
                  机器学习基础夯实系列-贝叶斯优化
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-21T21:42:34+08:00">
                2019-07-21
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/机器学习基础/" itemprop="url" rel="index">
                    <span itemprop="name">机器学习基础</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/21/ml-bo/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/07/21/ml-bo/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="贝叶斯优化小结"><a href="#贝叶斯优化小结" class="headerlink" title="贝叶斯优化小结"></a>贝叶斯优化小结</h2><h3 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h3><p>针对通用的函数求最小化的问题 $x<em>∗=argmin</em>{x∈X}f(x)$ ，如果定义域X是凸集和函数f是凸函数，可以采用凸优化的思路得到最优值,而针对非凸函数，f的一次运算需要大量的资源。通常此时可以采用贝叶斯优化的思路</p>
<h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>首先对f(x)有一个猜想的先验分布模型PF，然后利用后续新获取到的信息，来不断优化那个猜想模型，使模型越来越接近真实的分布</p>
<h3 id="主要方法"><a href="#主要方法" class="headerlink" title="主要方法"></a>主要方法</h3><h4 id="SMBO"><a href="#SMBO" class="headerlink" title="SMBO"></a>SMBO</h4><p><strong>Sequential model-based optimization (SMBO)</strong> 是贝叶斯优化的最简形式</p>
<p><img src="https://ask.qcloudimg.com/draft/1215004/k0e3uubws5.png" alt="img"></p>
<h4 id="主要过程"><a href="#主要过程" class="headerlink" title="主要过程"></a>主要过程</h4><p>1.输入步：Input: f黑盒子函数，X训练数据，S为<strong>Acquisition Function</strong>，M为是<strong>基于输入数据假设的模型，即已知的输入数据x都是在这个模型上的</strong>，可以用来假设的模型有很多种，例如随机森林，Tree Parzen Estimators，常见的比如GP高斯分布过程。</p>
<p>2.初始化步(构建示例)：初始化获取数据InitSamples(f,X), $D={(x_i,y<em>i)}</em>{i=1}^{n}$,其中每一个$y_i$为已知的, 即$y_i=f(x_i)$</p>
<p>3.迭代步：<br>3.1 $p(y|x,D) &lt;- FitModel(M, D)$ 使用猜想的分布模型M(比如高斯分布) 在特定的数据下进行训练。由于输入服从高斯分布，那么可以知道其预测也是服从高斯分布的。这里实质计算计算高斯分布的主要参数估计的均值和方差。</p>
<p>3.2.基于估计的假定模型M(高斯分布)去选择当前轮的输入$x_i$,基本思想就是<strong>选择收益最高的点</strong>。主要是利用<strong>Acquisition Function</strong>函数。acquisition function是一个权衡exploritation和exploration的函数。</p>
<p>1) Optimistic policies , 主要采用上限置信区间（upper confidence bound）。常用的如GP-UCB等方法</p>
<p>2) Information-based policies,主要思想是利用后验信息来进行选点。常用有Thompson sampling 和 entropy search。基于熵的方法感觉发展空间还比较大，有一些相关工作都有用到这个。</p>
<p>3) Portfolios of acquisition functions </p>
<p>这类方法就是将多种AC方法进行集成，最近的工作比如有ESP</p>
<p>4）<strong>Expected improvement(EI)</strong></p>
<p>可以在explore和explot之间平衡，explore时选择均值大的点,exploit选择方差大的点</p>
<p>5） Probability of improvement(POI)</p>
<p>新的采样能提升最大值的概率最大， MPI（maximum probability of improvement)，或P算法</p>
<p>3.3 使用新样本进行模型训练，此步骤比较耗时</p>
<p>3.4 训练实例的更新</p>
<h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>LDA、组合优化、自动机器学习、增强学习、气象、机器人等等</p>
<h3 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h3><p>1.<a href="https://blog.csdn.net/Snail_Ren/article/details/79005069" target="_blank" rel="external">https://blog.csdn.net/Snail_Ren/article/details/79005069</a></p>
<p>2.<a href="https://github.com/tobegit3hub/advisor" target="_blank" rel="external">https://github.com/tobegit3hub/advisor</a> Google内部的Vizier调参服务以及其开源实现Advisor项目</p>
<p>3.<a href="https://zhuanlan.zhihu.com/p/29779000" target="_blank" rel="external">https://zhuanlan.zhihu.com/p/29779000</a>  贝叶斯优化: 一种更好的超参数调优方式 </p>
<p>4.贝叶斯参数调优实战：<a href="https://www.jianshu.com/p/4c0cef6176fa" target="_blank" rel="external">https://www.jianshu.com/p/4c0cef6176fa</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/11/distill/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/07/11/distill/" itemprop="url">
                  知识蒸馏-Knowledge Distillation
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-11T10:21:23+08:00">
                2019-07-11
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/模型压缩/" itemprop="url" rel="index">
                    <span itemprop="name">模型压缩</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/模型压缩/Distillation/" itemprop="url" rel="index">
                    <span itemprop="name">Distillation</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/11/distill/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/07/11/distill/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="基础介绍"><a href="#基础介绍" class="headerlink" title="基础介绍"></a>基础介绍</h2><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>KD（Distilling the Knowledge），就是用teacher network的输出作为soft label(target)来训练一个student network。</p>
<ul>
<li><strong>Knowledge Distill是一种简单弥补分类问题监督信号不足的办法</strong>. 常见的监督信号表示hard target,是0-1的表示，而KD的表示从soft target学习，拥有<strong>不同类之间关系</strong>的信息（比如同时分类驴和马的时候，尽管某张图片是马，但是soft target就不会像hard target 那样只有马的index处的值为1，其余为0，而是在驴的部分也会有概率。）</li>
<li>知识蒸馏是一种模型压缩常见方法，用于模型压缩指的是在teacher-student框架中，将复杂、学习能力强的网络学到的特征表示“知识”蒸馏出来，传递给参数量小、学习能力弱的网络。</li>
</ul>
<h3 id="Loss表达"><a href="#Loss表达" class="headerlink" title="Loss表达"></a>Loss表达</h3><p>Loss的表达：$L=\alpha L<em>{soft}+(1−\alpha)L</em>{hard}$<br>distillation loss选择：平方距离，KL-divergence，cross entropy。</p>
<h3 id="核心结构："><a href="#核心结构：" class="headerlink" title="核心结构："></a>核心结构：</h3><p><img src="source/images/distill/image-20190710154101256.png" alt="image-20190710154101256"></p>
<h2 id="相关文章调研"><a href="#相关文章调研" class="headerlink" title="相关文章调研"></a>相关文章调研</h2><p>0.开山作：Hinton发表在NIPS2014文章</p>
<p>1.Attention Transfer：传递teacher网络的attention信息给student网络</p>
<p>attention transfer的目的是将teacher网络某层的这种spatial attention map传递给student网络，让student网络相应层的spatial attention map可以模仿teacher，从而达到知识蒸馏目的。</p>
<p>2.<strong>FSP matrix</strong></p>
<p>之前KD的teacher的某层的输出作为student的mimic目标，这篇文章将teacher网络层与层之间的关系作为student网络mimic的目标。</p>
<p>3.<strong>DarkRank: Accelerating Deep Metric Learning via Cross Sample Similarities</strong></p>
<p>一种适合度量学习（如检索，Re-id，人脸识别，图像聚类）的知识蒸馏方法。所传递的知识就是度量学习所度量的样本间相似度，用learn to rank来传递知识。</p>
<p>4.<strong>Neuron Selectivity Transfer</strong><br>文章:Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</p>
<p>teacher-student的knowledge transfer过程看作两者对应feature distribution matching，使用domian adaptation 常用方法<strong>MMD(最大平均差异)</strong>进行优化。<strong>(知识蒸馏本是一种同任务迁移学习)</strong></p>
<p><strong>使用MMD来使得student网络的神经元选择性特征分布（Neuron Selectivity Feature Distributions）mimic对应teacher的的这种分布</strong>。</p>
<p>5.<strong>Knowledge Distillation with Conditional Adversarial Networks</strong></p>
<p>Paper:Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks</p>
<p>6.<strong>Deep Mutual Learning</strong></p>
<p>Deep Mutual Learning(DML)与用于模型压缩的一般知识蒸馏不同的地方在于知识蒸馏是将预训练好的、不进行反向传播的“静态”teacher网络的知识单项传递给需要反向传播的”动态”student网络.</p>
<p>DML:Teacher 网络也要进行反向传播</p>
<p>7.<strong>Born Again Neural Networks</strong> 再生网络</p>
<p>核心思路：直接将teacher网络的prediction当作student网络的target，得到第一代student网络的prediction，然后传递给后一代，历经几代之后，将各代student网络的prediction ensemble.形成一个sequence of teaching selves。</p>
<p>8.在线蒸馏 codistillation</p>
<p>在分布式训练任务下，提出了一种替代标准SGD训练NN模型的方法codistillation，是一种非同步的算法，事实上有很多个Wieght的副本在独立训练，他可以有效“解决”机器增加但线性度不增加的问题，实验中还有一些数据表面可以比标准的SGD收敛更快</p>
<p>distill的思想，但是因为是重头训练，所以什么是teacher model呢？作者提出用所有模型的预测平均作为teacher model，然后作为soft target来训练每一个模型。</p>
<p>10.<a href="https://arxiv.org/abs/1904.09482" target="_blank" rel="external">Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding</a> 微软的文章</p>
<p><img src="source/images/distill/image-20190710163227607.png" alt="image-20190710163227607"></p>
<p>Teacher是多个，每个是一个MT-DNN，Student是一个小的MT-DNN，朴素的KD做法，效果比Teacher还好</p>
<p>Code:<a href="https://github.com/namisan/mt-dnn" target="_blank" rel="external">https://github.com/namisan/mt-dnn</a></p>
<p>发现有益的结论：Hinton的知识蒸馏的第一项权重相当大，这里发现大小差异不大。</p>
<p>问：Student的网络结构和单个Teacher是一样的？</p>
<p>11.Model Compression with Multi-Task Knowledge Distillation for Web-scale Question Answering System</p>
<p>Multi-task Knowledge Distillation Model </p>
<p>Student：3层BERT结构，使用BERT的参数进行初始化；Teacher: BERT-Base，个数是3个</p>
<p>在讨论部分提出了一个Enhanced Student Model with Two-Stage Multi-Task Knowledge Distillation，结构如下：</p>
<p><img src="source/images/distill/image-20190710172005310.png" alt="image-20190710172005310"></p>
<p>两阶段的范式：第一阶段使用soft label进行学习，第二阶段使用GT和soft label进行学习，效果比MKDM好，但比原始的都要差。</p>
<h2 id="实现问题思考"><a href="#实现问题思考" class="headerlink" title="实现问题思考"></a>实现问题思考</h2><p>1.student网络的具体形态的选择？很多方式，可以异质也可以同质的</p>
<p>2.teacher网络的结构，选择几个分类器，因为数据集上不同个数的分类器性能不一样</p>
<p>3.teacher预测错误的知识需要单独剔除吗？</p>
<p>4.当类别少的时候效果就不太显著，对于非分类问题也不适用？</p>
<p>常见的T-S形态</p>
<p>Case1:</p>
<p><strong>teacher</strong>：WRN-40-10</p>
<p><strong>student</strong>: WRN-10-4(CIFAR)/WRN-22-4(Imagenet32)</p>
<p>Case2:</p>
<p>Teacher:BERT-Base</p>
<p>Student:3层BERT / BiLSTMAttn+TextCNN</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/51563760" target="_blank" rel="external">简评 | 知识蒸馏（Knowledge Distillation）最新进展（一）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/53864403" target="_blank" rel="external">简评 | 知识蒸馏（Knowledge Distillation）最新进展（二）</a></li>
<li><a href="https://www.zhihu.com/question/50519680/answer/136406661" target="_blank" rel="external">如何理解soft target这一做法？</a></li>
<li>soft target的作用在于<strong>generalization</strong>。同dropout、L2 regularization、pre-train有相同作用。</li>
<li><a href="https://blog.csdn.net/nature553863/article/details/80568658" target="_blank" rel="external">知识蒸馏（Knowledge Distillation</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/09/tool-audio/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/07/09/tool-audio/" itemprop="url">
                  音频特征抽取
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-09T16:14:23+08:00">
                2019-07-09
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tool/" itemprop="url" rel="index">
                    <span itemprop="name">Tool</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Tool/Audio/" itemprop="url" rel="index">
                    <span itemprop="name">Audio</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/09/tool-audio/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/07/09/tool-audio/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h4 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h4><p>原材料：视频文件(signed 16-bit PCM)/音频</p>
<p>工具：VGGish/ffmpeg</p>
<p>任务：抽取音频文件的embedding语义向量.</p>
<p>下载地址：<a href="https://github.com/tensorflow/models/tree/master/research/audioset/vggish" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/audioset/vggish</a></p>
<h4 id="VGGish基础介绍"><a href="#VGGish基础介绍" class="headerlink" title="VGGish基础介绍"></a>VGGish基础介绍</h4><p>在<a href="https://link.zhihu.com/?target=https%3A//research.google.com/audioset/download.html" target="_blank" rel="external">AudioSet</a>是Google发行的声音版ImageNet上训练得到预训练模型，该模型可以将音频文件抽取为128维度的语义向量，除了直接抽取特征外，还可以进行针对特定的任务进行FineTuning操作。</p>
<h5 id="VGGish-vs-VGG"><a href="#VGGish-vs-VGG" class="headerlink" title="VGGish vs VGG"></a>VGGish vs VGG</h5><p>VGGish是 <a href="https://arxiv.org/abs/1409.1556" target="_blank" rel="external">VGG</a>的变体，含有11个权重层，具体有如下改变：</p>
<ul>
<li>输入大小修改为96x64，log mel spectrogram的音频输入</li>
<li>去掉了最后一组的conv和pool层，有4组结构而不是5组e.</li>
<li>全联接层(compact embedding层)不是想image那样使用1000而是使用的是128维度的.</li>
</ul>
<h5 id="VGGish依赖包"><a href="#VGGish依赖包" class="headerlink" title="VGGish依赖包"></a>VGGish依赖包</h5><ul>
<li><a href="http://www.numpy.org/" target="_blank" rel="external"><code>numpy</code></a></li>
<li><a href="http://www.scipy.org/" target="_blank" rel="external"><code>scipy</code></a></li>
<li><a href="http://resampy.readthedocs.io/en/latest/" target="_blank" rel="external"><code>resampy</code></a></li>
<li><a href="http://www.tensorflow.org/" target="_blank" rel="external"><code>tensorflow</code></a></li>
<li><a href="https://pythonhosted.org/six/" target="_blank" rel="external"><code>six</code></a></li>
<li><a href="https://pysoundfile.readthedocs.io/" target="_blank" rel="external"><code>pysoundfile</code></a></li>
</ul>
<h5 id="VGGish文件结构："><a href="#VGGish文件结构：" class="headerlink" title="VGGish文件结构："></a>VGGish文件结构：</h5><ul>
<li><code>vggish_slim.py</code>: Model definition in TensorFlow Slim notation.</li>
<li><code>vggish_params.py</code>: Hyperparameters.</li>
<li><code>vggish_input.py</code>: Converter from audio waveform into input examples.</li>
<li><code>mel_features.py</code>: Audio feature extraction helpers.</li>
<li><code>vggish_postprocess.py</code>: Embedding postprocessing.</li>
<li><code>vggish_inference_demo.py</code>: Demo of VGGish in inference mode.</li>
<li><code>vggish_train_demo.py</code>: Demo of VGGish in training mode.</li>
<li><code>vggish_smoke_test.py</code>: Simple test of a VGGish installation</li>
</ul>
<h4 id="VGGish使用介绍"><a href="#VGGish使用介绍" class="headerlink" title="VGGish使用介绍"></a>VGGish使用介绍</h4><p>音频文件：signed 16-bit PCM samples</p>
<p>使用示例: vggish_inference_demo.py</p>
<ul>
<li><p>计算log mel spectrogram()<br>examples_batch = vggish_input.wavfile_to_examples(wav_file)</p>
</li>
<li><p>vggish抽取 </p>
<p>features_tensor = sess.graph.get_tensor_by_name(vggish_params.INPUT_TENSOR_NAME)<br>embedding_tensor = sess.graph.get_tensor_by_name(vggish_params.OUTPUT_TENSOR_NAME)[embedding_batch] = sess.run([embedding_tensor], feed_dict={features_tensor: examples_batch})</p>
</li>
<li><p>后处理</p>
<p>PCA变换+8bit定点化</p>
</li>
</ul>
<p>VGGish was trained with audio features computed as follows:</p>
<ul>
<li>All audio is resampled to 16 kHz mono.</li>
<li>A spectrogram is computed using magnitudes of the Short-Time Fourier Transform with a window size of 25 ms, a window hop of 10 ms, and a periodic Hann window.</li>
<li>A mel spectrogram is computed by mapping the spectrogram to 64 mel bins covering the range 125-7500 Hz.</li>
<li>A stabilized log mel spectrogram is computed by applying log(mel-spectrum + 0.01) where the offset is used to avoid taking a logarithm of zero.</li>
<li>These features are then framed into non-overlapping examples of 0.96 seconds, where each example covers 64 mel bands and 96 frames of 10 ms each.</li>
</ul>
<h4 id="视频抽取音频"><a href="#视频抽取音频" class="headerlink" title="视频抽取音频"></a>视频抽取音频</h4><p>ffmpeg -y -i  xxx.mp4  -ar 16000 -ac  1  xxx.wav</p>
<p>主要参数含义：</p>
<p>-y: 覆盖输出的文件</p>
<p>-ar rate            set audio sampling rate (in Hz)<br>-ac channels        set number of audio channels</p>
<h4 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h4><ul>
<li>Gemmeke, J. et. al., <a href="https://research.google.com/pubs/pub45857.html" target="_blank" rel="external">AudioSet: An ontology and human-labelled dataset for audio events</a>, ICASSP 2017</li>
<li>Hershey, S. et. al., <a href="https://research.google.com/pubs/pub45611.html" target="_blank" rel="external">CNN Architectures for Large-Scale Audio Classification</a>, ICASSP 2017</li>
<li><a href="https://github.com/tensorflow/models/tree/master/research/audioset" target="_blank" rel="external">https://github.com/tensorflow/models/tree/master/research/audioset</a></li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/08/vat/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/07/08/vat/" itemprop="url">
                  虚拟对抗训练
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-08T20:37:48+08:00">
                2019-07-08
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GAN/" itemprop="url" rel="index">
                    <span itemprop="name">GAN</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/GAN/虚拟对抗训练/" itemprop="url" rel="index">
                    <span itemprop="name">虚拟对抗训练</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/08/vat/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/07/08/vat/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>虚拟对抗训练(VAT): 全称为Virtual Adversarial Training,核心思想是在监督学习中对抗部分，采取的措施可以是加噪音增强鲁棒性、对抗具体的分布等，联合对抗部分loss一起训练。</p>
<p>1.Word Embedding Perturbation for Sentence Classification</p>
<p>在答案选择、关系分类、情感分类上，在word embedding上面实现对抗</p>
<p>code:<a href="https://github.com/zhangdongxu/word-embedding-perturbation" target="_blank" rel="external">https://github.com/zhangdongxu/word-embedding-perturbation</a></p>
<p>2.<a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1808.03908" target="_blank" rel="external">Adversarial Personalized Ranking for Recommendation</a></p>
<p>VAT是在对抗训练的基础上将监督学习模型扩展到半监督学习模型，同时模型在同向噪声鲁棒性提高到可以在异向噪声具有鲁棒性，在监督和半监督条件下都取得不错的实验结果。</p>
<p><strong>3. Virtual Adversarial Training: a Regularization Method for Supervised and Semi-supervised Learning</strong></p>
<p>利用对抗的思想，要求模型对一个样本在施加对抗性噪声前后给出尽可能相同的预测值，从而对模型施加 smooth regularization，以此利用无标注样本进行半监督学习。论文仅用 100 个标注 MNIST 样本取得 1.36% 的测试误差，仅用 4000 个标注 CIFAR 样本取得 13.15% 的测试误差。</p>
<p><a href="https://github.com/takerum/vat_tf" target="_blank" rel="external">Code</a></p>
<p>4.<strong>Adversarial Dropout for Supervised and Semi-supervised Learning</strong></p>
<p>Virtual Adverarial Training的变种，原来在 input data 上加对抗干扰，本文在网络中间层进行对抗性 dropout，取得了与 VAT 接近的半监督训练效果，配合原始 VAT 一起在 CIFAR 和 SVHN 上取得 state-of-the-art 的半监督学习性能</p>
<p>5.Distributional Smoothing with Virtual Adversarial Training</p>
<p>局部分布性平滑(LDS) 的方法，这是统计模型的一个新的光滑概念，可以用作正则化术语来促进模型分布的平滑</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/02/RecSys-Advance-BERT/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/07/02/RecSys-Advance-BERT/" itemprop="url">
                  推荐系统前沿系列-BERT方法
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-02T23:56:36+08:00">
                2019-07-02
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/BERT/" itemprop="url" rel="index">
                    <span itemprop="name">BERT</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/02/RecSys-Advance-BERT/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/07/02/RecSys-Advance-BERT/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="BERT在推荐系统的梳理"><a href="#BERT在推荐系统的梳理" class="headerlink" title="BERT在推荐系统的梳理"></a>BERT在推荐系统的梳理</h3><p>1.Pre-training of Context-aware Item Representation for Next Basket Recommendation</p>
<p>作者：Jingxuan Yang1, Jun Xu2, Jianzhuo Tong1, Sheng Gao1, Jun Guo1, Jirong Wen2 1Beijng University of Posts and Telecommunications</p>
<p>单位：北邮、中国人民大学</p>
<p>ArXiv版本号:1904.12604.pdf</p>
<p>2.BERT4Rec: Sequential Recommendation with Bidirectional Encoder Representations from Transformer</p>
<p>作者：Fei Sun, Jun Liu, Jian Wu, Changhua Pei, Xiao Lin, Wenwu Ou, and Peng Jiang</p>
<p>单位：阿里巴巴</p>
<p>ArXiv版本号:1904.06690.pdf</p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/07/01/RecSys-Advance-GAN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/07/01/RecSys-Advance-GAN/" itemprop="url">
                  推荐系统前沿系列-生成对抗网络概览
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-07-01T23:28:46+08:00">
                2019-07-01
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/07/01/RecSys-Advance-GAN/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/07/01/RecSys-Advance-GAN/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="GAN在推荐中应用梳理"><a href="#GAN在推荐中应用梳理" class="headerlink" title="GAN在推荐中应用梳理"></a>GAN在推荐中应用梳理</h3><h4 id="IRGAN"><a href="#IRGAN" class="headerlink" title="IRGAN"></a>IRGAN</h4><ul>
<li>G：给定query，预测document.D:给定<query,document>预测得分<br>针对推荐场景：G：给定User.生成/预测item,D:给定<user,item>预测得分<br>这对D的离散问题，采样方式，使用PG的方式，学习过程-baseline,使用critic进行评估</user,item></query,document></li>
<li>如何扩展到pair-wise的使用场景？</li>
</ul>
<h4 id="GraphGAN"><a href="#GraphGAN" class="headerlink" title="GraphGAN"></a>GraphGAN</h4><p>G：给定顶点，生成其他的顶点<br>D：判断顶点对是否有这样的关系</p>
<h4 id="CFGAN"><a href="#CFGAN" class="headerlink" title="CFGAN"></a>CFGAN</h4><ul>
<li>直觉：<strong>discrete item index generation</strong>问题</li>
</ul>
<h4 id="RAGAN"><a href="#RAGAN" class="headerlink" title="RAGAN"></a>RAGAN</h4><p>GAN生成的数据做labeled进行数据增强</p>
<ul>
<li><p>直觉：数据稀疏的问题？</p>
</li>
<li><p>方法：设计出RAGAN/RAGAN+bias treat,如何寻找负向？排序低、未召回</p>
<p>one-class collaborative  Filtering (OCCF) framework</p>
</li>
</ul>
<h4 id="APR"><a href="#APR" class="headerlink" title="APR"></a>APR</h4><p>模型更加鲁棒，APR：MF+APR</p>
<h4 id="Adversarial-Recommendation-Attack-of-the-Learned-Fake-Users"><a href="#Adversarial-Recommendation-Attack-of-the-Learned-Fake-Users" class="headerlink" title="Adversarial Recommendation: Attack of the Learned Fake Users"></a>Adversarial Recommendation: Attack of the Learned Fake Users</h4><p>来源：<a href="https://arxiv.org/pdf/1809.08336.pdf" target="_blank" rel="external">https://arxiv.org/pdf/1809.08336.pdf</a></p>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
      

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/06/28/RecSys-Advance-RL/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="数据娃嚼AI">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/towan.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="数据娃嚼AI">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                
                <a class="post-title-link" href="/2019/06/28/RecSys-Advance-RL/" itemprop="url">
                  推荐系统前沿系列-强化学习方法概览
                </a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-06-28T23:16:30+08:00">
                2019-06-28
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/" itemprop="url" rel="index">
                    <span itemprop="name">推荐系统</span>
                  </a>
                </span>

                
                
                  ， 
                
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/推荐系统/DRL/" itemprop="url" rel="index">
                    <span itemprop="name">DRL</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2019/06/28/RecSys-Advance-RL/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2019/06/28/RecSys-Advance-RL/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="主要文章"><a href="#主要文章" class="headerlink" title="主要文章"></a>主要文章</h2><ul>
<li>Generative Adversarial User Model for Reinforcement Learning Based Recommendation System，蚂蚁金服，ICML 2019</li>
<li><a href="https://arxiv.org/pdf/1811.05869.pdf" target="_blank" rel="external">Large-scale Interactive Recommendation with Tree-structured Policy Gradient</a><br>树型策略梯度(tree-structured policy gradient)的强化学习模型TPGR,<a href="http://vlambda.com/wz_wwE6WGcjsz.html" target="_blank" rel="external">相关报道</a></li>
<li><a href="https://arxiv.org/abs/1802.06501" target="_blank" rel="external">Recommendations with Negative Feedback via Pairwise Deep Reinforcement Learning</a>  KDD 2018</li>
<li><p><a href="https://arxiv.org/pdf/1805.02343" target="_blank" rel="external">Deep Reinforcement Learning for Page-wise Recommendations</a>, JD, RecSys 2018</p>
</li>
<li><p><a href="https://arxiv.org/abs/1803.00710" target="_blank" rel="external">Reinforcement Learning to Ranking in E-commerce Search Engine:Formalization, Analysis, and Application</a> KDD 2018</p>
</li>
<li><a href="https://arxiv.org/pdf/1805.02343.pdf" target="_blank" rel="external">A Deep Reinforcement Learning Framework for News Recommendation</a> JD, WWW 2018 </li>
<li>Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology，IJCAI 2019</li>
<li>Top-K Off-Policy Correction for a REINFORCE Recommender System，WSDM 2019</li>
<li>Deep Reinforcement Learning for Sponsored Search Real-time Bidding， Alibaba, DQN</li>
</ul>
<h2 id="重点解读"><a href="#重点解读" class="headerlink" title="重点解读"></a>重点解读</h2><h3 id="Generative-Adversarial-User-Model-for-Reinforcement-Learning-Based-Recommendation-System，蚂蚁金服，ICML-2019"><a href="#Generative-Adversarial-User-Model-for-Reinforcement-Learning-Based-Recommendation-System，蚂蚁金服，ICML-2019" class="headerlink" title="Generative Adversarial User Model for Reinforcement Learning Based Recommendation System，蚂蚁金服，ICML 2019"></a>Generative Adversarial User Model for Reinforcement Learning Based Recommendation System，蚂蚁金服，ICML 2019</h3><ul>
<li><p>主要贡献点：</p>
<ul>
<li>用户的行为模型+reward统一由一个minmax的框架来学习</li>
<li>以这套模型为环境，开发了一个级联的DQN的方法线性复杂度的解决组合选择action的问题</li>
</ul>
</li>
<li><p>值得借鉴点：</p>
<ul>
<li>强化学习的五元组建模方法</li>
<li>对抗训练的建模方式。G：基于用户的历史行为序列生成当前的行为概率；D：尝试从生成的行为序列和真实的区分出来。</li>
<li>行为序列两种建模方式：LSTM/Position Weight</li>
<li>贪心方式的级联的DQN的处理方式</li>
</ul>
</li>
</ul>

          
        
      
    </div>

    <div>
      
    </div>

    <div>
      
    </div>


    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right"></i></a>
  </nav>


          
          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/towan.jpg"
               alt="数据娃嚼AI" />
          <p class="site-author-name" itemprop="name">数据娃嚼AI</p>
           
              <p class="site-description motion-element" itemprop="description">天下之至柔，驰骋天下之至坚</p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">20</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              <a href="/categories/index.html">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              <a href="/tags/index.html">
                <span class="site-state-item-count">25</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/htw2012" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/u/3061921383/" target="_blank" title="Weibo">
                  
                    <i class="fa fa-fw fa-weibo"></i>
                  
                  Weibo
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy;  2017 - 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">数据娃嚼AI</span>
</div>


<div class="powered-by">
  <a class="theme-link" href="https://hexo.io">Hexo</a>
</div>

<div class="theme-info">
  theme -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Mist
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"htw2012"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.id = 'duoshuo-script';
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>

  
    
    
    <script src="/lib/ua-parser-js/dist/ua-parser.min.js?v=0.7.9"></script>
    <script src="/js/src/hook-duoshuo.js"></script>
  













  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


  

</body>
</html>
